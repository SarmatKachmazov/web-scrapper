{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импортирую библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs \n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "from time import sleep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://coinmarketcap.com\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 1\n",
    "while num != 6:\n",
    "    with open(f'/Users/sarmat/Desktop/agency/readyLinks{num}') as file: # Загружаю json с ссылками\n",
    "        readyLinks = json.load(file)\n",
    "    countBase = 0\n",
    "    #count = 0\n",
    "    for category_name, category_href in readyLinks.items(): # Раскрываю словарь с ссылками \n",
    "\n",
    "        #if count < 102:\n",
    "            req = requests.get(url = category_href, headers = headers) # Делаю запрос\n",
    "            src = req.text\n",
    "        \n",
    "            with open(f'data/base{num}/{category_name}.html', 'w') as file:\n",
    "                file.write(src)\n",
    "\n",
    "            with open(f'data/base{num}/{category_name}.html') as file:\n",
    "                src = file.read()\n",
    "\n",
    "            soup = bs(src, 'lxml') # Создаю суп \n",
    "\n",
    "            # Формируем необходимые заголовки\n",
    "                \n",
    "            rank = 'Rank'\n",
    "            company = 'Company'\n",
    "            status = 'Status'\n",
    "            tg_chat = 'Telegram Chat'\n",
    "            contact = 'Contact Person'\n",
    "            site = 'URL'\n",
    "            telegram = 'Telegram'\n",
    "            twitter = 'Twitter'\n",
    "            reddit = 'Reddit'\n",
    "            dc_chat = 'Discord Chat'\n",
    "            medium = 'Medium'\n",
    "            discord = 'Discord'\n",
    "\n",
    "            \n",
    "            if countBase == 0:\n",
    "                with open(f'data/base{num}/base.csv', 'w', encoding = 'utf-8') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(\n",
    "                        (\n",
    "                            rank,\n",
    "                            company,\n",
    "                            status,\n",
    "                            tg_chat,\n",
    "                            contact,\n",
    "                            site,\n",
    "                            telegram,\n",
    "                            twitter,\n",
    "                            reddit,\n",
    "                            dc_chat,\n",
    "                            medium,\n",
    "                            discord  \n",
    "                        )\n",
    "                    )\n",
    "                countBase += 1  \n",
    "            \n",
    "            # Собираем данные !!!\n",
    "            rank = soup.find(class_ ='namePill namePillPrimary').text\n",
    "            try:\n",
    "                company = soup.find(class_ = 'sc-1d5226ca-0 gNMZTD').text\n",
    "            except:\n",
    "                company = soup.find(class_ = 'sc-1d5226ca-0 gNMZTD tooltip').text\n",
    "\n",
    "            print('Компания на очереди:' + company)\n",
    "            print()\n",
    "            \n",
    "\n",
    "            # Находит ссылку на сайт !!!\n",
    "            all_url = soup.find(class_ = 'content').find_all('li')\n",
    "\n",
    "            for item in all_url:\n",
    "                url1 = item.find_all(class_ = 'buttonName')\n",
    "                for j in url1:\n",
    "                    if '.' in j.text:\n",
    "                        site = j.text\n",
    "            #print(site)\n",
    "            \n",
    "            # Оставшиеся ссылки !!!\n",
    "            linkUrl = [] # Ссылка\n",
    "            linkName = [] # Название ссылки\n",
    "            names = [] # Для сбора исключений\n",
    "            # Счетчики\n",
    "            countReddit = 0\n",
    "            countTelegram = 0\n",
    "            countMedium = 0\n",
    "            countTwitter = 0\n",
    "            countDiscord = 0\n",
    "            countTelegramChat = 0\n",
    "            tMe = 0\n",
    "            for a in soup.find_all('a', class_ = 'modalLink', href = True):\n",
    "                try:\n",
    "                    if a.text == 'Reddit' and countReddit == 0:\n",
    "                        countReddit += 1\n",
    "                        linkUrl.append(a['href'])\n",
    "                        linkName.append(a.text)\n",
    "                    if a.text == 't.me' and countTelegram == 0:\n",
    "                        countTelegram += 1\n",
    "                        linkUrl.append(a['href'])\n",
    "                        linkName.append(a.text)\n",
    "                    if a.text == 't.me' and tMe == 0:\n",
    "                        tMe += 1\n",
    "                        linkUrl.append(a['href'])\n",
    "                        linkName.append(a.text)\n",
    "                    if a.text == 'Twitter' and countTwitter == 0:\n",
    "                        countTwitter += 1\n",
    "                        linkUrl.append(a['href'])\n",
    "                        linkName.append(a.text)\n",
    "                    if a.text == 'medium.com' and countMedium == 0:\n",
    "                        countMedium += 1\n",
    "                        linkUrl.append(a['href'])\n",
    "                        linkName.append(a.text)\n",
    "                    if a.text == 'Discord' and countDiscord == 0:\n",
    "                        countDiscord += 1\n",
    "                        linkUrl.append(a['href'])\n",
    "                        linkName.append(a.text)\n",
    "                    if a.text == 'Chat' and countTelegramChat == 0:\n",
    "                        countTelegramChat += 1\n",
    "                        linkUrl.append(a['href'])\n",
    "                        linkName.append(a.text)\n",
    "                    else:\n",
    "                        names.append(a.text)\n",
    "                except:\n",
    "                    print('Error:' + linkName) \n",
    "\n",
    "            allLinksInfo = dict(zip(linkName, linkUrl))\n",
    "            allLinksInfo['URL'] = site\n",
    "\n",
    "            print(names)\n",
    "            print()\n",
    "            print(len(linkUrl) - len(linkName))\n",
    "            print()\n",
    "            print(allLinksInfo)\n",
    "            print()\n",
    "\n",
    "            # a!!!\n",
    "            reddit = 'None'\n",
    "            site = 'None'\n",
    "            telegram = 'None'\n",
    "            discord = 'None'\n",
    "            medium = 'None'\n",
    "            tg_chat = 'None'\n",
    "            contact = 'None'\n",
    "            twitter = 'None'\n",
    "            dc_chat = 'None'\n",
    "            status = 'Не обработан'\n",
    "            \n",
    "\n",
    "            for item in allLinksInfo:\n",
    "                if item == 'Reddit':\n",
    "                    reddit = allLinksInfo.get('Reddit')\n",
    "                elif item == 'URL':\n",
    "                    site = allLinksInfo.get('URL')\n",
    "                elif item == 't.me':\n",
    "                    telegram = allLinksInfo.get('t.me')\n",
    "                elif item == 'Twitter':\n",
    "                    twitter = allLinksInfo.get('Twitter')\n",
    "                elif item == 'Discord':\n",
    "                    discord = allLinksInfo.get('Discord')\n",
    "                elif item == 'medium.com':\n",
    "                    medium = allLinksInfo.get('medium.com')\n",
    "                elif item == 'Chat':\n",
    "                    tg_chat = allLinksInfo.get('Chat')\n",
    "\n",
    "                \n",
    "            with open(f'data/base{num}/base.csv', 'a', encoding = 'utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(\n",
    "                    (\n",
    "                        rank,\n",
    "                        company,\n",
    "                        status,\n",
    "                        tg_chat,\n",
    "                        contact,\n",
    "                        site,\n",
    "                        telegram,\n",
    "                        twitter,\n",
    "                        reddit,\n",
    "                        dc_chat,\n",
    "                        medium,\n",
    "                        discord  \n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            #count += 1\n",
    "            sleep(random.randrange(1, 3))\n",
    "    num += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
